{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPYQLzPfE_ub"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/NLP/"
      ],
      "metadata": {
        "id": "Oso2fmDMFQsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "INPUT_PATH = './yelp_academic_dataset_review.json'\n",
        "DATASET_CSV_PATH = 'yelp_reviews_gen.csv'\n",
        "PATH_TO_YELP_REVIEWS = INPUT_PATH"
      ],
      "metadata": {
        "id": "tQ40VuIZGb9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downlaoding the CSV files -- sharing the CSV between the CNN and LSTM notebooks\n",
        "! gdown --id 1tO13aN1nnJqupL-JkydxBg7H5ET4y0HM"
      ],
      "metadata": {
        "id": "sGbDZNAbGjuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_data_df = pd.read_csv(DATASET_CSV_PATH)\n",
        "print(\"Columns in the original dataset:\\n\")\n",
        "print(top_data_df.columns)"
      ],
      "metadata": {
        "id": "HaejpfpkGqJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os import statvfs_result\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "print(\"Number of rows per star rating:\")\n",
        "print(top_data_df['stars'].value_counts())\n",
        "\n",
        "# Function to assign sentiment\n",
        "def get_sentiment(stars, bi=True, tri=False):\n",
        "    # assert stars >= 1 and stars <= 5 and type(stars) == int\n",
        "    if bi:\n",
        "        if stars <=2:\n",
        "            return 0\n",
        "        else:\n",
        "            return 1\n",
        "    elif tri:\n",
        "        if stars <= 2:\n",
        "            return -1\n",
        "        elif stars == 3:\n",
        "            return 0\n",
        "        else:\n",
        "            return 1\n",
        "    else:\n",
        "        return int(stars)\n",
        "\n",
        "# Function to map stars to sentiment\n",
        "# def map_sentiment(stars_received):\n",
        "#     if stars_received <= 2:\n",
        "#         return -1\n",
        "#     elif stars_received == 3:\n",
        "#         return 0\n",
        "#     else:\n",
        "#         return 1\n",
        "# Mapping stars to sentiment into three categories\n",
        "top_data_df['sentiment'] = [ get_sentiment(x, bi=True) for x in top_data_df['stars']]\n",
        "# Plotting the sentiment distribution\n",
        "plt.figure()\n",
        "pd.value_counts(top_data_df['sentiment']).plot.bar(title=\"Sentiment distribution in df\")\n",
        "plt.xlabel(\"Sentiment\")\n",
        "plt.ylabel(\"No. of rows in df\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c-7kQaK6HOLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to retrieve top few number of each category\n",
        "def get_top_data(top_n = 5000):\n",
        "    data_positive_df = top_data_df[top_data_df['sentiment'] == 1].head(top_n)\n",
        "    data_negative_df = top_data_df[top_data_df['sentiment'] == 0].head(top_n)\n",
        "\n",
        "    final_data_df = pd.concat([data_positive_df, data_negative_df])\n",
        "    return final_data_df\n",
        "\n",
        "    # top_data_df_positive = top_data_df[top_data_df['sentiment'] == 1].head(top_n)\n",
        "    # top_data_df_negative = top_data_df[top_data_df['sentiment'] == -1].head(top_n)\n",
        "    # top_data_df_neutral = top_data_df[top_data_df['sentiment'] == 0].head(top_n)\n",
        "    # top_data_df_small = pd.concat([top_data_df_positive, top_data_df_negative, top_data_df_neutral])\n",
        "    # return top_data_df_small\n",
        "\n",
        "# Function call to get the top 10000 from each sentiment\n",
        "reduced_dataset = get_top_data(top_n=10000)\n",
        "\n",
        "# After selecting top few samples of each sentiment\n",
        "print(\"After segregating and taking equal number of rows for each sentiment:\")\n",
        "print(reduced_dataset['sentiment'].value_counts())\n",
        "reduced_dataset.head(10)"
      ],
      "metadata": {
        "id": "0M9_DRp_JSlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample - Removing the stop words\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "print(remove_stopwords(\"Restaurant had a really good service!!\"))"
      ],
      "metadata": {
        "id": "hXZ8RUU-JYDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text column to get the new column 'tokenized_text'\n",
        "from gensim.utils import simple_preprocess\n",
        "reduced_dataset['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in reduced_dataset['text']] \n",
        "print(reduced_dataset['tokenized_text'].head(10))"
      ],
      "metadata": {
        "id": "d9A-xBbgJgmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.parsing.porter import PorterStemmer\n",
        "porter_stemmer = PorterStemmer()\n",
        "# Get the stemmed_tokens\n",
        "reduced_dataset['stemmed_tokens'] = [[porter_stemmer.stem(word) for word in tokens] for tokens in reduced_dataset['tokenized_text'] ]\n",
        "reduced_dataset['stemmed_tokens'].head(10)"
      ],
      "metadata": {
        "id": "4o5qPMOnJmIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Train Test Split Function\n",
        "def split_train_test(top_data_df_small, test_size=0.3, shuffle_state=True):\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(reduced_dataset[['business_id', 'cool', 'date', 'funny', 'review_id', 'stars', 'text', 'useful', 'user_id', 'stemmed_tokens']], \n",
        "                                                        reduced_dataset['sentiment'], \n",
        "                                                        shuffle=shuffle_state,\n",
        "                                                        test_size=test_size, \n",
        "                                                        random_state=15)\n",
        "    print(\"Value counts for Train sentiments\")\n",
        "    print(Y_train.value_counts())\n",
        "    print(\"Value counts for Test sentiments\")\n",
        "    print(Y_test.value_counts())\n",
        "    print(type(X_train))\n",
        "    print(type(Y_train))\n",
        "    X_train = X_train.reset_index()\n",
        "    X_test = X_test.reset_index()\n",
        "    Y_train = Y_train.to_frame()\n",
        "    Y_train = Y_train.reset_index()\n",
        "    Y_test = Y_test.to_frame()\n",
        "    Y_test = Y_test.reset_index()\n",
        "    print(X_train.head())\n",
        "    return X_train, X_test, Y_train, Y_test\n",
        "\n",
        "# Call the train_test_split\n",
        "X_train, X_test, Y_train, Y_test = split_train_test(reduced_dataset)"
      ],
      "metadata": {
        "id": "njyGyyYjJpix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torch\n",
        "# Use cuda if present\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(f\"Device available for running: {device}\")"
      ],
      "metadata": {
        "id": "a3rRQfSZJ1j2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "size = 500\n",
        "window = 3\n",
        "min_count = 1\n",
        "workers = 3\n",
        "sg = 1\n",
        "OUTPUT_FOLDER = 'OpData'\n",
        "# Function to train word2vec model\n",
        "def make_word2vec_model(reduced_dataset, padding=True, sg=1, min_count=1, size=500, workers=3, window=3):\n",
        "    if  padding:\n",
        "        print(len(reduced_dataset))\n",
        "        temp_df = pd.Series(reduced_dataset['stemmed_tokens']).values\n",
        "        temp_df = list(temp_df)\n",
        "        temp_df.append(['pad'])\n",
        "        word2vec_file = OUTPUT_FOLDER + '/models/'+'word2vec_' + str(size) + '_PAD.model'\n",
        "    else:\n",
        "        temp_df = reduced_dataset['stemmed_tokens']\n",
        "        word2vec_file = OUTPUT_FOLDER + '/models/' + 'word2vec_' + str(size) + '.model'\n",
        "    w2v_model = Word2Vec(temp_df, min_count = min_count, vector_size=size, workers = workers, window = window, sg = sg)\n",
        "\n",
        "    # w2v_model.save(word2vec_file)\n",
        "    return w2v_model, word2vec_file\n",
        "\n",
        "# Train Word2vec model\n",
        "w2vmodel, word2vec_file = make_word2vec_model(reduced_dataset, padding=True, sg=sg, min_count=min_count, size=size, workers=workers, window=window)"
      ],
      "metadata": {
        "id": "nBwc6eF-J5mO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_sen_len = reduced_dataset.stemmed_tokens.map(len).max()\n",
        "padding_idx = w2vmodel.wv.key_to_index['pad']\n",
        "def make_word2vec_vector_cnn(sentence):\n",
        "    padded_X = [padding_idx for i in range(max_sen_len)]\n",
        "    i = 0\n",
        "    for word in sentence:\n",
        "        if word not in w2vmodel.wv.key_to_index:\n",
        "            padded_X[i] = 0\n",
        "            print(word)\n",
        "        else:\n",
        "            padded_X[i] = w2vmodel.wv.key_to_index[word]\n",
        "        i += 1\n",
        "    return torch.tensor(padded_X, dtype=torch.long, device=device).view(1, -1)"
      ],
      "metadata": {
        "id": "hXpfSO8SSNaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get the output tensor\n",
        "def make_target(label):\n",
        "    if label == -1:\n",
        "        return torch.tensor([0], dtype=torch.long, device=device)\n",
        "    elif label == 0:\n",
        "        return torch.tensor([1], dtype=torch.long, device=device)\n",
        "    else:\n",
        "        return torch.tensor([2], dtype=torch.long, device=device)"
      ],
      "metadata": {
        "id": "ZL3R2gmPMzdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "EMBEDDING_SIZE = 500\n",
        "NUM_FILTERS = 10\n",
        "\n",
        "class LSTMSentimentClassifier(nn.Module):\n",
        "    def __init__(self, no_layers, output_dim, hidden_dim, embedding_dim, drop_prob=0.5):\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.no_layers = no_layers\n",
        "        w2vmodel = gensim.models.KeyedVectors.load(OUTPUT_FOLDER + 'models/' + 'word2vec_500_PAD.model')\n",
        "        weights = w2vmodel.wv\n",
        "\n",
        "        # With pretrained embeddings\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights.vectors), padding_idx=w2vmodel.wv.key_to_index['pad'])\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=self.hidden_dim,\n",
        "                            num_layers=no_layers, batch_first=True)\n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "        # linear and sigmoid layer\n",
        "        self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
        "        self.sig = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "      batch_size = x.size(0)\n",
        "      embeds = self.embedding(x)\n",
        "\n",
        "      lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "      lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "\n",
        "      # dropout and fully connected layer\n",
        "      out = self.dropout(lstm_out)\n",
        "      out = self.fc(out)\n",
        "\n",
        "      # sigmoid function\n",
        "      sig_out = self.sig(out)\n",
        "\n",
        "      # reshape to be batch_size first\n",
        "      sig_out = sig_out.view(batch_size, -1)\n",
        "\n",
        "      sig_out = sig_out[:, -1]  # get last batch of labels\n",
        "\n",
        "      # return last sigmoid output and hidden state\n",
        "      return sig_out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size, device):\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        h0 = torch.zeros((self.no_layers, batch_size, self.hidden_dim)).to(device)\n",
        "        c0 = torch.zeros((self.no_layers, batch_size, self.hidden_dim)).to(device)\n",
        "        hidden = (h0, c0)\n",
        "        return hidden\n"
      ],
      "metadata": {
        "id": "5UER0BnwNFYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders for training & validation sets\n",
        "train_data = TensorDataset(torch.from_numpy(x_train_pad), torch.from_numpy(y_train))\n",
        "valid_data = TensorDataset(torch.from_numpy(x_test_pad), torch.from_numpy(y_test))\n",
        "batch_size = 50\n",
        "\n",
        "# data shuffling\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "RvQ2PnmaJp3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# == sample params == \n",
        "clipping_quotient = 5\n",
        "num_epochs = 50\n",
        "valid_loss_min = np.Inf\n",
        "layers_count = 2\n",
        "embedding_dimensions = 500\n",
        "output_dimensions = 1\n",
        "hidden_dimensions = 256"
      ],
      "metadata": {
        "id": "8XDgI5paJ4uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classes = 2 ['positive', 'negative']\n",
        "learning_rate=0.001\n",
        "loss_func = nn.BCELoss()\n",
        "model = LSTMSentimentClassifier(layers_count,output_dimensions,hidden_dimensions,embedding_dimensions)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "epoch_tr_loss = list()\n",
        "epoch_vl_loss = list()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    tr_losses = []\n",
        "    train_acc = 0.0\n",
        "    print(\"Epoch\" + str(epoch + 1))\n",
        "    model.train()\n",
        "\n",
        "    h = model.init_hidden(batch_size, device)\n",
        "\n",
        "    # training data\n",
        "    for inputs, labels in train_loader:\n",
        "        if device == 'cuda':\n",
        "          inputs, labels = inputs.to(device), labels.to(device)\n",
        "        h = tuple([each.data for each in h])\n",
        "        output, h = model(inputs, h)\n",
        "        loss_gradient = loss_func(output.squeeze(), labels.float())\n",
        "        loss_gradient.backward()\n",
        "        tr_losses += loss_gradient.item()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), clipping_quotient)\n",
        "        optimizer.step()\n",
        "    \n",
        "    # validation data\n",
        "    h = model.init_hidden(batch_size, device)\n",
        "    v_losses = []\n",
        "    v_acc = 0.0\n",
        "    model.eval()\n",
        "    for inputs, labels in valid_loader:\n",
        "      v_h = tuple([each.data for each in h])\n",
        "      if device == 'cuda':\n",
        "          inputs, labels = inputs.to(device), labels.to(device)\n",
        "          output, v_h = model(inputs, v_h)\n",
        "          val_loss = loss_func(output.squeeze(), labels.float())\n",
        "          v_losses += val_loss.item()\n",
        "    \n",
        "    epoch_tr_loss += np.mean(tr_losses)\n",
        "    epoch_vl_loss += np.mean(vl_losses)\n",
        "    print(f'train_loss : {np.mean(tr_losses)} val_loss : {np.mean(vl_losses)}'))\n",
        "\n",
        "# model_path = OUTPUT_FOLDER + 'models/sentiment_model_30ep.pt'\n",
        "torch.save(model, OUTPUT_FOLDER +'/' +'lstm_500_with_padding.pt')"
      ],
      "metadata": {
        "id": "3qpTaCmAKBU_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}